<div align="center">
  
# ğŸ™ï¸ Podcast Transcriber

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

å°†æ’­å®¢éŸ³é¢‘è½¬å½•ä¸ºæ–‡å­—çš„å¼€æºå·¥å…·ï¼Œæ”¯æŒæœ¬åœ° Whisper è½¬å½•ï¼ŒGPU åŠ é€Ÿã€‚

</div>

---

<a name="ä¸­æ–‡"></a>
## ä¸­æ–‡æ–‡æ¡£

### åŠŸèƒ½ç‰¹ç‚¹

- **ğŸ¤ æœ¬åœ°è½¬å½•**: ä½¿ç”¨ Faster-Whisper æœ¬åœ°è½¬å½•ï¼Œæ— éœ€ä¾èµ–äº‘æœåŠ¡
- **ğŸš€ GPU åŠ é€Ÿ**: æ”¯æŒ NVIDIA GPU (CUDA)ï¼Œ4090 è½¬å½• 1 å°æ—¶éŸ³é¢‘ä»…éœ€ 2-3 åˆ†é’Ÿ
- **ğŸ”— å¤šå¹³å°æ”¯æŒ**: æ”¯æŒå°å®‡å®™ã€Apple Podcastsã€RSS è®¢é˜…æºã€ç›´æ¥éŸ³é¢‘é“¾æ¥
- **ğŸ“± å“åº”å¼è®¾è®¡**: æ”¯æŒæ¡Œé¢å’Œç§»åŠ¨ç«¯è®¿é—®
- **ğŸ’¾ ä¸€é”®ä¸‹è½½**: è½¬å½•å®Œæˆåå¯ç›´æ¥ä¸‹è½½ Markdown æ ¼å¼çš„æ–‡å­—ç¨¿

### æ€§èƒ½å¯¹æ¯”

| è®¾å¤‡ | æ¨¡å‹ | 10åˆ†é’ŸéŸ³é¢‘ | 1å°æ—¶éŸ³é¢‘ |
|------|------|-----------|----------|
| CPU (M2 Mac) | base | ~3-5åˆ†é’Ÿ | ~20-30åˆ†é’Ÿ |
| 4090 GPU | base | ~5-10ç§’ | ~30-60ç§’ |
| 4090 GPU | large-v3 | ~15-30ç§’ | ~2-3åˆ†é’Ÿ |

---

### å¿«é€Ÿå¼€å§‹ (æœ¬åœ° CPU ç‰ˆ)

#### ç¯å¢ƒè¦æ±‚

- Node.js 18+
- Python 3.8+
- ffmpeg

#### å®‰è£…æ­¥éª¤

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/avacx/podcast.git
cd podcast

# å®‰è£… Node.js ä¾èµ–
npm install

# åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£… Faster-Whisper
pip install faster-whisper

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env

# å¯åŠ¨æœåŠ¡
npm start
```

è®¿é—® http://localhost:3000

---

### GPU æœåŠ¡å™¨éƒ¨ç½² (Ubuntu + NVIDIA GPU)

æ¨èä½¿ç”¨ NVIDIA GPU æœåŠ¡å™¨ï¼ˆå¦‚ 4090ï¼‰è·å¾—æœ€ä½³æ€§èƒ½ã€‚

#### æ–¹æ³•ä¸€ï¼šè‡ªåŠ¨éƒ¨ç½²è„šæœ¬

```bash
# 1. ä¸Šä¼ é¡¹ç›®åˆ°æœåŠ¡å™¨
scp -r podcast user@your-server:~/

# 2. SSH ç™»å½•æœåŠ¡å™¨
ssh user@your-server

# 3. è¿è¡Œéƒ¨ç½²è„šæœ¬
cd ~/podcast
chmod +x deploy/setup-gpu-server.sh
./deploy/setup-gpu-server.sh
```

#### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨éƒ¨ç½²

```bash
# å®‰è£…ç³»ç»Ÿä¾èµ–
sudo apt update
sudo apt install -y curl git ffmpeg python3 python3-pip python3-venv nodejs npm

# ç¡®è®¤ NVIDIA é©±åŠ¨
nvidia-smi

# å…‹éš†é¡¹ç›®
git clone https://github.com/avacx/podcast.git
cd podcast

# å®‰è£…ä¾èµ–
npm install
python3 -m venv venv
source venv/bin/activate
pip install faster-whisper

# é…ç½® GPU ç¯å¢ƒå˜é‡
cp deploy/.env.gpu.example .env

# å¯åŠ¨æœåŠ¡
npm start
```

#### æœ¬åœ°è®¿é—®è¿œç¨‹æœåŠ¡å™¨

éƒ¨ç½²å®Œæˆåï¼Œåœ¨æœ¬åœ°æµè§ˆå™¨è®¿é—®ï¼š
```
http://æœåŠ¡å™¨IP:3000
```

ç¡®ä¿é˜²ç«å¢™å¼€æ”¾ 3000 ç«¯å£ï¼š
```bash
sudo ufw allow 3000
```

---

### ç¯å¢ƒå˜é‡é…ç½®

#### CPU ç‰ˆæœ¬ (.env)
```env
PORT=3000
USE_LOCAL_WHISPER=true
WHISPER_MODEL=base
WHISPER_DEVICE=cpu
WHISPER_COMPUTE_TYPE=int8
```

#### GPU ç‰ˆæœ¬ (.env)
```env
PORT=3000
HOST=0.0.0.0
USE_LOCAL_WHISPER=true
WHISPER_MODEL=large-v3
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

#### æ¨¡å‹é€‰æ‹©

| æ¨¡å‹ | ç²¾åº¦ | é€Ÿåº¦ | æ˜¾å­˜å ç”¨ |
|------|------|------|---------|
| tiny | ä½ | æœ€å¿« | ~1GB |
| base | ä¸­ | å¿« | ~1GB |
| small | ä¸­é«˜ | ä¸­ç­‰ | ~2GB |
| medium | é«˜ | è¾ƒæ…¢ | ~5GB |
| large-v3 | æœ€é«˜ | æ…¢ | ~10GB |

4090 æ˜¾å¡æ¨èä½¿ç”¨ `large-v3` è·å¾—æœ€ä½³è½¬å½•è´¨é‡ã€‚

---

### é¡¹ç›®ç»“æ„

```
podcast/
â”œâ”€â”€ public/                 # å‰ç«¯é¡µé¢
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ script.js
â”œâ”€â”€ server/                 # åç«¯æœåŠ¡
â”‚   â”œâ”€â”€ index.js           # Express æœåŠ¡å™¨
â”‚   â”œâ”€â”€ whisper_transcribe.py  # Whisper è½¬å½•è„šæœ¬
â”‚   â””â”€â”€ services/          # ä¸šåŠ¡é€»è¾‘
â”œâ”€â”€ deploy/                 # éƒ¨ç½²ç›¸å…³
â”‚   â”œâ”€â”€ setup-gpu-server.sh    # GPU æœåŠ¡å™¨éƒ¨ç½²è„šæœ¬
â”‚   â”œâ”€â”€ .env.gpu.example       # GPU é…ç½®æ¨¡æ¿
â”‚   â””â”€â”€ DEPLOY_GPU.md          # è¯¦ç»†éƒ¨ç½²æ–‡æ¡£
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡æ¨¡æ¿
â””â”€â”€ package.json
```

---

### å¸¸è§é—®é¢˜

**Q: æç¤º `venv/bin/python: No such file or directory`**

A: éœ€è¦åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒï¼š
```bash
python3 -m venv venv
source venv/bin/activate
pip install faster-whisper
```

**Q: GPU ç‰ˆæœ¬æç¤º CUDA ä¸å¯ç”¨**

A: æ£€æŸ¥ NVIDIA é©±åŠ¨å’Œ CUDAï¼š
```bash
nvidia-smi
python3 -c "import torch; print(torch.cuda.is_available())"
```

**Q: é¦–æ¬¡è½¬å½•å¾ˆæ…¢**

A: é¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½ Whisper æ¨¡å‹æ–‡ä»¶ï¼Œåç»­ä¼šä½¿ç”¨ç¼“å­˜ã€‚

---

### License

Apache 2.0 License

---

<a name="english"></a>
## English Documentation

### Features

- **ğŸ¤ Local Transcription**: Uses Faster-Whisper for local transcription, no cloud dependency
- **ğŸš€ GPU Acceleration**: Supports NVIDIA GPU (CUDA), 4090 transcribes 1-hour audio in 2-3 minutes
- **ğŸ”— Multi-Platform**: Supports Xiaoyuzhou, Apple Podcasts, RSS feeds, direct audio URLs
- **ğŸ“± Responsive Design**: Works on desktop and mobile
- **ğŸ’¾ One-Click Download**: Download transcripts in Markdown format

### Quick Start (Local CPU)

```bash
# Clone
git clone https://github.com/avacx/podcast.git
cd podcast

# Install dependencies
npm install
python3 -m venv venv
source venv/bin/activate
pip install faster-whisper

# Configure
cp .env.example .env

# Start
npm start
```

Visit http://localhost:3000

### GPU Server Deployment

See [deploy/DEPLOY_GPU.md](deploy/DEPLOY_GPU.md) for detailed instructions.

```bash
# Quick deploy on Ubuntu + NVIDIA GPU
chmod +x deploy/setup-gpu-server.sh
./deploy/setup-gpu-server.sh
```

### Environment Variables

```env
# GPU Configuration
WHISPER_MODEL=large-v3
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

### License

Apache 2.0 License
