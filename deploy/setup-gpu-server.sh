#!/bin/bash
# Ubuntu 22.04 + NVIDIA 4090D éƒ¨ç½²è„šæœ¬
# ä½¿ç”¨æ–¹æ³•: chmod +x setup-gpu-server.sh && ./setup-gpu-server.sh

set -e

echo "ðŸš€ å¼€å§‹éƒ¨ç½² Podcast Transcriber (GPU ç‰ˆæœ¬)"
echo "============================================"

# 1. ç³»ç»Ÿæ›´æ–°
echo "ðŸ“¦ æ›´æ–°ç³»ç»ŸåŒ…..."
sudo apt update && sudo apt upgrade -y

# 2. å®‰è£…åŸºç¡€ä¾èµ–
echo "ðŸ“¦ å®‰è£…åŸºç¡€ä¾èµ–..."
sudo apt install -y curl git ffmpeg python3 python3-pip python3-venv nodejs npm

# 3. æ£€æŸ¥ NVIDIA é©±åŠ¨
echo "ðŸ” æ£€æŸ¥ NVIDIA é©±åŠ¨..."
if ! command -v nvidia-smi &> /dev/null; then
    echo "âŒ æœªæ£€æµ‹åˆ° NVIDIA é©±åŠ¨ï¼Œè¯·å…ˆå®‰è£…é©±åŠ¨"
    echo "   è¿è¡Œ: sudo apt install nvidia-driver-535"
    exit 1
fi
nvidia-smi
echo "âœ… NVIDIA é©±åŠ¨æ­£å¸¸"

# 4. æ£€æŸ¥ CUDA
echo "ðŸ” æ£€æŸ¥ CUDA..."
if ! command -v nvcc &> /dev/null; then
    echo "âš ï¸ æœªæ£€æµ‹åˆ° CUDAï¼Œæ­£åœ¨å®‰è£… CUDA Toolkit..."
    # å®‰è£… CUDA 12.x
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
    sudo dpkg -i cuda-keyring_1.1-1_all.deb
    sudo apt update
    sudo apt install -y cuda-toolkit-12-4
    rm cuda-keyring_1.1-1_all.deb
    
    # æ·»åŠ çŽ¯å¢ƒå˜é‡
    echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
    echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
    source ~/.bashrc
fi
echo "âœ… CUDA å·²å®‰è£…"

# 5. å®‰è£… cuDNN (Faster-Whisper éœ€è¦)
echo "ðŸ“¦ æ£€æŸ¥ cuDNN..."
if ! ldconfig -p | grep -q libcudnn; then
    echo "âš ï¸ æ­£åœ¨å®‰è£… cuDNN..."
    sudo apt install -y libcudnn8 libcudnn8-dev
fi
echo "âœ… cuDNN å·²å®‰è£…"

# 6. å…‹éš†æˆ–æ›´æ–°é¡¹ç›®
PROJECT_DIR="$HOME/podcast-transcriber"
if [ -d "$PROJECT_DIR" ]; then
    echo "ðŸ“‚ æ›´æ–°çŽ°æœ‰é¡¹ç›®..."
    cd "$PROJECT_DIR"
    git pull
else
    echo "ðŸ“‚ å…‹éš†é¡¹ç›®..."
    git clone https://github.com/avacx/podcast.git "$PROJECT_DIR"
fi

cd "$PROJECT_DIR"

# 7. å®‰è£… Node.js ä¾èµ–
echo "ðŸ“¦ å®‰è£… Node.js ä¾èµ–..."
npm install

# 8. åˆ›å»º Python è™šæ‹ŸçŽ¯å¢ƒ
echo "ðŸ åˆ›å»º Python è™šæ‹ŸçŽ¯å¢ƒ..."
python3 -m venv venv
source venv/bin/activate

# 9. å®‰è£… Python ä¾èµ– (GPU ç‰ˆæœ¬)
echo "ðŸ“¦ å®‰è£… Faster-Whisper (CUDA ç‰ˆæœ¬)..."
pip install --upgrade pip
pip install faster-whisper

# éªŒè¯ CUDA æ”¯æŒ
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" 2>/dev/null || echo "PyTorch CUDA check skipped"

# 10. åˆ›å»º GPU ç‰ˆæœ¬çš„ .env æ–‡ä»¶
echo "âš™ï¸ åˆ›å»ºé…ç½®æ–‡ä»¶..."
cat > .env << 'EOF'
# Server Configuration
PORT=3000
HOST=0.0.0.0

# Whisper Configuration (GPU)
USE_LOCAL_WHISPER=true
WHISPER_MODEL=large-v3
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16

# Optional: OpenAI API (disabled for now)
# OPENAI_API_KEY=your-key
# OPENAI_BASE_URL=https://api.openai.com/v1
EOF

echo "âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º"

# 11. é¢„ä¸‹è½½ Whisper æ¨¡åž‹
echo "ðŸ“¥ é¢„ä¸‹è½½ Whisper large-v3 æ¨¡åž‹ (çº¦ 3GB)..."
python3 -c "
from faster_whisper import WhisperModel
print('æ­£åœ¨ä¸‹è½½ large-v3 æ¨¡åž‹...')
model = WhisperModel('large-v3', device='cuda', compute_type='float16')
print('âœ… æ¨¡åž‹ä¸‹è½½å®Œæˆ')
"

# 12. åˆ›å»º systemd æœåŠ¡
echo "ðŸ”§ åˆ›å»ºç³»ç»ŸæœåŠ¡..."
sudo tee /etc/systemd/system/podcast-transcriber.service > /dev/null << EOF
[Unit]
Description=Podcast Transcriber Service
After=network.target

[Service]
Type=simple
User=$USER
WorkingDirectory=$PROJECT_DIR
Environment=PATH=$PROJECT_DIR/venv/bin:/usr/local/cuda/bin:/usr/bin
ExecStart=/usr/bin/node server/index.js
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable podcast-transcriber
sudo systemctl start podcast-transcriber

echo ""
echo "============================================"
echo "âœ… éƒ¨ç½²å®Œæˆï¼"
echo "============================================"
echo ""
echo "ðŸ“ æœåŠ¡çŠ¶æ€: sudo systemctl status podcast-transcriber"
echo "ðŸ“ æŸ¥çœ‹æ—¥å¿—: sudo journalctl -u podcast-transcriber -f"
echo "ðŸ“ é‡å¯æœåŠ¡: sudo systemctl restart podcast-transcriber"
echo ""
echo "ðŸŒ è®¿é—®åœ°å€: http://$(hostname -I | awk '{print $1}'):3000"
echo ""
echo "âš ï¸ å¦‚æžœéœ€è¦å¤–ç½‘è®¿é—®ï¼Œè¯·ç¡®ä¿é˜²ç«å¢™å¼€æ”¾ 3000 ç«¯å£:"
echo "   sudo ufw allow 3000"
echo ""
